{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mu vs 1Ar39\n",
    "We see that the cnn model struggles in prediction of mu vs 1ar39.\n",
    "The idea is to train a single model for each class of ar39, in order to later combine their prediction in a final outcome (ensemble).\n",
    "\n",
    "In this notebook, we propose simple features on group of slices (_quadrant_) to see if with a restricted number of features, a linear model can be trained. Let see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [25, 15]\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stddev_idslices_fun(row):\n",
    "    # given an array of aquisitions, it return the std of activated slices\n",
    "    # it create a populaion of slice ids, and compute stddev on them\n",
    "    id_population = [item for id_list in [[i] * int(row[i]) for i in range(len(row))] for item in id_list]\n",
    "    if id_population:    # check if the list of slice ids is not empty\n",
    "        return np.std(id_population)\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_quadrant_features(df):\n",
    "    # split aquisitions in 4 \"shifted\" quadrant\n",
    "    nslices = 72\n",
    "    quadrant_width = nslices // 2   # idea: spread is within 30 slices (it depends on the sampling trick)\n",
    "    nshiftings = 4                  # arbitrary grain\n",
    "    shift = nslices // nshiftings   # derived\n",
    "    df = df.iloc[:, 2:nslices+2]   # skip the first two fields (edep, pedetected)\n",
    "    # create quadrants\n",
    "    df_quadrants = []\n",
    "    for i_shift in range(nshiftings):\n",
    "        assert(i_shift*shift < nslices)\n",
    "        quadrant = df.iloc[:, i_shift*shift:i_shift*shift + quadrant_width]\n",
    "        if i_shift*shift + quadrant_width >= nslices:\n",
    "            quadrant = pd.concat([quadrant, df.iloc[:, :i_shift*shift + quadrant_width - nslices]], axis=1)\n",
    "        df_quadrants.append(quadrant)\n",
    "    # compute stddev and meanPE for each quadrant\n",
    "    for quadrant in df_quadrants:\n",
    "        quadrant[\"stdslices\"] = quadrant.apply(stddev_idslices_fun, axis=1)\n",
    "        quadrant[\"meanpe\"] = quadrant.apply(lambda row: row[:36].mean(), axis=1)\n",
    "    # aggregate the features in a single dataset\n",
    "    features = pd.DataFrame()\n",
    "    for i, quadrant in enumerate(df_quadrants):\n",
    "        features = pd.concat([features, quadrant[[\"stdslices\", \"meanpe\"]]], axis=1)\n",
    "    features.columns = [\"{}_{}\".format(col, j) for j in range(len(df_quadrants)) for col in [\"stdslices\", \"meanpe\"]]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Loaded 6473 low-energy muons, 6500 single Ar39\n"
     ]
    }
   ],
   "source": [
    "filein = os.path.join(\"data\", \"Muons\", \"MarginalMuons_wt_0ar39_cut265PE.csv\")\n",
    "muons = pd.read_csv(filein, index_col=False)\n",
    "#muons[\"std\"] = muons.apply(lambda row: np.std([it for l in [[i] * int(row[2+i]) for i in range(72)] for it in l]), axis=1)\n",
    "\n",
    "filein = os.path.join(\"data\", \"Ar39\", \"dataset6500\", \"Ar39_1Pileup_cut265PE_n6500.csv\")\n",
    "ar39_1 = pd.read_csv(filein, index_col=False)\n",
    "#ar39_1[\"std\"] = ar39_1.apply(lambda row: np.std([it for l in [[i] * int(row[2+i]) for i in range(72)] for it in l]), axis=1)\n",
    "\n",
    "print(\"[Info] Loaded {} low-energy muons, {} single Ar39\".format(len(muons), len(ar39_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luigi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/luigi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "muon_features = produce_quadrant_features(muons)\n",
    "ar39_1_features = produce_quadrant_features(ar39_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import roc_curve, plot_roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# models\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train easy model logistic regression\n",
    "muon_features[\"y\"] = 1\n",
    "ar39_1_features[\"y\"] = 0\n",
    "data = pd.concat([muon_features, ar39_1_features], axis=0)\n",
    "X, y = data.iloc[:, :-1], data.iloc[:, -1]\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "#to_bytes_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \n",
    "         \"Decision Tree\", \"Random Forest\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"Logistic Regression\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "model_template   = \"[Info] Model: {}\"\n",
    "result_template  = \"[Info] Complete training in {:.3f} seconds.\\n\"\n",
    "result_template += \"[Result] Score: {:.3f}, Accuracy: {:.3f}, Precision: {:.3f}, Recall: {:.3f}\"\n",
    "cv_template      = \"[Detail] Confusion Matrix:\\n\"\n",
    "cv_template     += \"\\tAr39\\tMu\\n\"\n",
    "cv_template     += \"\\tPred\\tPred\\n\"\n",
    "cv_template     += \"----------------------\\n\"\n",
    "cv_template     += \"Ar39\\t{}\\t{}\\n\"\n",
    "cv_template     += \"Mu\\t{}\\t{}\\n\"\n",
    "cv_template     += \"----------------------\\n\"\n",
    "# Start kfold cross-validation\n",
    "n_folds = 10\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
    "evaluations = []\n",
    "for train_ids, test_ids in kf.split(X):\n",
    "    X_train, X_test = X[train_ids], X[test_ids]\n",
    "    y_train, y_test = y[train_ids], y[test_ids]\n",
    "    # iterate over classifiers\n",
    "    iteration_evaluations = []\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        print(model_template.format(name))\n",
    "        start = time.time()\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_time = time.time() - start\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        print(result_template.format(train_time, score, accuracy, precision, recall))\n",
    "        #print(cv_template.format(tn, fp, fn, tp))\n",
    "        iteration_evaluations.append({'accuracy': accuracy, 'precision': precision, 'recall': recall})\n",
    "    evaluations.append(iteration_evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = np.zeros(len(classifiers))\n",
    "precisions = np.zeros(len(classifiers))\n",
    "recalls = np.zeros(len(classifiers))\n",
    "for iteration in evaluations:\n",
    "    for i in range(len(classifiers)):\n",
    "        accuracies[i] += iteration[i][\"accuracy\"]\n",
    "        precisions[i] += iteration[i][\"precision\"]\n",
    "        recalls[i] += iteration[i][\"recall\"]\n",
    "\n",
    "accuracies /= len(evaluations)\n",
    "precisions /= len(evaluations)\n",
    "recalls /= len(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(len(names)), precisions, label=\"Purity (Precision)\")\n",
    "plt.bar(np.arange(len(names)), recalls, bottom=precisions, label=\"Efficiency (Recall)\")\n",
    "plt.xticks(np.arange(len(names)), names)\n",
    "plt.legend()\n",
    "plt.title(\"Purity/Efficiency averaged over 10-Fold Cross-Validation\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Stacked Purity,Efficiency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = classifiers[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(y_train)), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap on features engineering:\n",
    "- split the 72 readouts in 4 shifted quadrants: [0:36], [15:51], [36:72], [51:15]\n",
    "- for each of this quadrant, compute: mean PE detected, std deviation of population of active slices\n",
    "- the second one: population means [id] * PE[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_std = ar39_1_perquad[[\"q1_std\", \"q2_std\", \"q3_std\", \"q4_std\"]].max(axis=1)<7.1\n",
    "mask_meanpe = ar39_1_perquad[[\"q1_meanpe\", \"q2_meanpe\", \"q3_meanpe\", \"q4_meanpe\"]].max(axis=1)<2\n",
    "print(\"TN: {}\".format(len(ar39_1_perquad[(mask_std) & (mask_meanpe)])))\n",
    "print(\"FP: {}\".format(len(ar39_1_perquad[~(mask_std) | ~(mask_meanpe)])))\n",
    "\n",
    "mask_std = muons_perquad[[\"q1_std\", \"q2_std\", \"q3_std\", \"q4_std\"]].max(axis=1)>=7.1\n",
    "mask_meanpe = muons_perquad[[\"q1_meanpe\", \"q2_meanpe\", \"q3_meanpe\", \"q4_meanpe\"]].max(axis=1)>=2\n",
    "print(\"FN: {}\".format(len(muons_perquad[~(mask_std) | ~(mask_meanpe)])))\n",
    "print(\"TP: {}\".format(len(muons_perquad[(mask_std) & (mask_meanpe)])))\n",
    "\n",
    "print(\"Tot Ar39: {}\".format(len(ar39_1_perquad)))\n",
    "print(\"Tot Mu: {}\".format(len(muons_perquad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filein = os.path.join(\"data\", \"Ar39\", \"dataset100000\", \"Ar39_1Pileup_cut265PE_n100000.csv\")\n",
    "all_ar39_1 = pd.read_csv(filein, index_col=False)\n",
    "all_ar39_1_slices = all_ar39_1.iloc[:, 2:nslices+2]    # skip initial fields\n",
    "all_ar39_1_quadrants = []\n",
    "for i_shift in range(nshiftings):\n",
    "    assert(i_shift*shift < nslices)\n",
    "    quadrant = all_ar39_1_slices.iloc[:, i_shift*shift:i_shift*shift + quadrant_width]\n",
    "    if i_shift*shift + quadrant_width >= nslices:\n",
    "        quadrant = pd.concat([quadrant, all_ar39_1_slices.iloc[:, :i_shift*shift + quadrant_width - nslices]], axis=1)\n",
    "    all_ar39_1_quadrants.append(quadrant)\n",
    "for quadrant in all_ar39_1_quadrants:\n",
    "    quadrant[\"stdslices\"] = quadrant.apply(stddev_idslices_fun, axis=1)\n",
    "    quadrant[\"meanpe\"] = quadrant.apply(lambda row: row[:36].mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ar39_1_features = pd.DataFrame()\n",
    "for i, quadrant in enumerate(all_ar39_1_quadrants):\n",
    "    all_ar39_1_features = pd.concat([all_ar39_1_features, quadrant[[\"stdslices\", \"meanpe\"]]], axis=1)\n",
    "all_ar39_1_features.columns = [\"{}_{}\".format(col, j) for j in range(len(all_ar39_1_quadrants)) for col in [\"stdslices\", \"meanpe\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ar39_1_features[\"y\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ar39, y_test_ar39 = all_ar39_1_features.iloc[:, :-1], all_ar39_1_features.iloc[:, -1]\n",
    "X_test_ar39 = StandardScaler().fit_transform(X_test_ar39)\n",
    "X_test_ar39 = np.array(X_test_ar39)\n",
    "y_test_ar39 = np.array(y_test_ar39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree.score(X_test_ar39, y_test_ar39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
