{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "plt.rcParams['figure.figsize'] = [25, 15]\n",
    "plt.rcParams.update({'font.size': 45})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe_from_files(dirin, fileprefix, max_files=100):\n",
    "    import glob\n",
    "    files = glob.glob(os.path.join(dirin, fileprefix))\n",
    "    print(\"[Info] Loading {} files wt prefix:\\n{}\".format(len(files), fileprefix))\n",
    "    df = pd.read_csv(files[0], comment='#', index_col=False)\n",
    "    for file in files[1:max_files]:\n",
    "        print(\".\", end='')\n",
    "        dftmp = pd.read_csv(file, comment='#', index_col=False)\n",
    "        df = pd.concat([df, dftmp])\n",
    "    print(\"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inner_slices, n_outer_slices = 12, 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Loading 100 files wt prefix:\n",
      "Snapshot1Ar39*\n",
      "...................................................................................................\n",
      "[Info] Loading 100 files wt prefix:\n",
      "Snapshot2Ar39*\n",
      "...................................................................................................\n",
      "[Info] Loading 100 files wt prefix:\n",
      "Snapshot3Ar39*\n",
      "...................................................................................................\n",
      "[Info] Loading 100 files wt prefix:\n",
      "Snapshot4Ar39*\n",
      "...................................................................................................\n",
      "[Info] Loading 100 files wt prefix:\n",
      "Snapshot5Ar39*\n",
      "...................................................................................................\n",
      "[Info] Loaded all 1, 2, 3, 4, 5 Ar39 Pileups in 15.32 seconds\n",
      "[Info] Loading 100 files wt prefix:\n",
      "SnapshotMuons*\n",
      "...................................................................................................\n",
      "[Info] Loaded all Muons in 0.74 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load Ar39\n",
    "init = time.time()\n",
    "dirin = os.path.join(\"..\", \"..\", \"Data\", \"OutputProcessing\", \"UnseenTestData_11_10_2020\", \"Ar39\", \"Ar39_Snapshots\")\n",
    "\n",
    "fileprefix = \"Snapshot1Ar39*\"\n",
    "dfall1ar39 = load_dataframe_from_files(dirin, fileprefix)\n",
    "\n",
    "fileprefix = \"Snapshot2Ar39*\"\n",
    "dfall2ar39 = load_dataframe_from_files(dirin, fileprefix)\n",
    "\n",
    "fileprefix = \"Snapshot3Ar39*\"\n",
    "dfall3ar39 = load_dataframe_from_files(dirin, fileprefix)\n",
    "\n",
    "fileprefix = \"Snapshot4Ar39*\"\n",
    "dfall4ar39 = load_dataframe_from_files(dirin, fileprefix)\n",
    "\n",
    "fileprefix = \"Snapshot5Ar39*\"\n",
    "dfall5ar39 = load_dataframe_from_files(dirin, fileprefix)\n",
    "\n",
    "print(\"[Info] Loaded all 1, 2, 3, 4, 5 Ar39 Pileups in {:.2f} seconds\".format(time.time() - init))\n",
    "\n",
    "# Load Muons\n",
    "init = time.time()\n",
    "dirin = os.path.join(\"..\", \"..\", \"Data\", \"OutputProcessing\", \"UnseenTestData_11_10_2020\", \"Muons\", \"Muons_Snapshots\")\n",
    "\n",
    "fileprefix = \"SnapshotMuons*\"\n",
    "dfallmu = load_dataframe_from_files(dirin, fileprefix)\n",
    "print(\"[Info] Loaded all Muons in {:.2f} seconds\".format(time.time() - init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Test Set Composition:\n",
      "\tMuons: 15454\n",
      "\t1Ar39: 548429\n",
      "\t2Ar39: 274191\n",
      "\t3Ar39: 182778\n",
      "\t4Ar39: 137071\n",
      "\t5Ar39: 109648\n"
     ]
    }
   ],
   "source": [
    "print(\"[Info] Test Set Composition:\")\n",
    "print(\"\\tMuons: {}\".format(len(dfallmu)))\n",
    "print(\"\\t1Ar39: {}\".format(len(dfall1ar39)))\n",
    "print(\"\\t2Ar39: {}\".format(len(dfall2ar39)))\n",
    "print(\"\\t3Ar39: {}\".format(len(dfall3ar39)))\n",
    "print(\"\\t4Ar39: {}\".format(len(dfall4ar39)))\n",
    "print(\"\\t5Ar39: {}\".format(len(dfall5ar39)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7d62cf5c0a6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rf_10est_5depth.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"..\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mu_vs_pileups\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_20200911-111046\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.oncerepeat1.epoch02-valacc0.91889\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Info] Loaded models in {:.2f} seconds\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/ML4NP/env/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   raise IOError(\n",
      "\u001b[0;32m~/Development/ML4NP/env/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m   model = tf_load.load_internal(\n\u001b[0;32m--> 121\u001b[0;31m       path, options=options, loader_cls=KerasObjectLoader)\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/ML4NP/env/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls)\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m--> 633\u001b[0;31m                             ckpt_options)\n\u001b[0m\u001b[1;32m    634\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         raise FileNotFoundError(\n",
      "\u001b[0;32m~/Development/ML4NP/env/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models_to_reconstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasObjectLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# Now that the node object has been fully loaded, and the checkpoint has\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/ML4NP/env/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options)\u001b[0m\n\u001b[1;32m    120\u001b[0m     self._concrete_functions = (\n\u001b[1;32m    121\u001b[0m         function_deserialization.load_function_def_library(\n\u001b[0;32m--> 122\u001b[0;31m             meta_graph.graph_def.library))\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/ML4NP/env/lib/python3.6/site-packages/tensorflow/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36mload_function_def_library\u001b[0;34m(library, load_shared_name_suffix)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;31m# signatures, respectively).  function_spec is set up later by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;31m# recreate_function(); and arg_keywords by setup_bare_concrete_function().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcreteFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/ML4NP/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func_graph, attrs, shared_func_graph, function_spec)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     \u001b[0;31m# FuncGraph directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m     self._delayed_rewrite_functions = _DelayedRewriteGradientFunctions(\n\u001b[0;32m-> 1542\u001b[0;31m         func_graph, self._attrs, self._garbage_collector)\n\u001b[0m\u001b[1;32m   1543\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_order_tape_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_higher_order_tape_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/ML4NP/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[1;32m    604\u001b[0m     self._inference_function = _EagerDefinedFunction(\n\u001b[1;32m    605\u001b[0m         \u001b[0m_inference_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         self._func_graph.inputs, self._func_graph.outputs, attrs)\n\u001b[0m\u001b[1;32m    607\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/ML4NP/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_EagerDefinedFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/ML4NP/env/lib/python3.6/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mensure_initialized\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m   \u001b[0;34m\"\"\"Initialize the context.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m   \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/ML4NP/env/lib/python3.6/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_tfrt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m           \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ContextOptionsSetTfrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_tfrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mcontext_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_NewContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_DeleteContextOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "import tensorflow as tf\n",
    "\n",
    "init = time.time()\n",
    "rf = load(\"rf_10est_5depth.joblib\")\n",
    "cnn = tf.keras.models.load_model(os.path.join(\"..\", \"mu_vs_pileups\", \"models\", \"model_20200911-111046\", \"model.oncerepeat1.epoch02-valacc0.91889\"))\n",
    "print(\"[Info] Loaded models in {:.2f} seconds\".format(time.time() - init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "dfallmu[\"group\"] = 0\n",
    "dfall1ar39[\"group\"] = 1\n",
    "dfall2ar39[\"group\"] = 2\n",
    "dfall3ar39[\"group\"] = 3\n",
    "dfall4ar39[\"group\"] = 4\n",
    "dfall5ar39[\"group\"] = 5\n",
    "dfallmu[\"y\"] = 1\n",
    "dfall1ar39[\"y\"] = 0\n",
    "dfall2ar39[\"y\"] = 0\n",
    "dfall3ar39[\"y\"] = 0\n",
    "dfall4ar39[\"y\"] = 0\n",
    "dfall5ar39[\"y\"] = 0\n",
    "dfallar39 = pd.concat([dfall1ar39, dfall2ar39, dfall3ar39, dfall4ar39, dfall5ar39], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Features\n",
    "def pe_detected(row):\n",
    "    return np.sum(row.to_numpy())\n",
    "\n",
    "def nr_active_slices(row):\n",
    "    return np.nonzero(row.to_numpy())[0].shape[0]\n",
    "\n",
    "def mean_npe(row):\n",
    "    return np.mean(row.to_numpy())\n",
    "\n",
    "def mean_npe_active(row):\n",
    "    rownp = row.to_numpy()\n",
    "    rownonzero = np.nonzero(rownp)[0]\n",
    "    return np.mean(rownp[rownonzero]) if rownonzero.shape[0]>0 else -1\n",
    "\n",
    "def std_npe(row):\n",
    "    return np.std(row.to_numpy())\n",
    "\n",
    "def std_npe_active(row):\n",
    "    rownp = row.to_numpy()\n",
    "    rownonzero = np.nonzero(rownp)[0]\n",
    "    return np.std(rownp[rownonzero]) if rownonzero.shape[0]>0 else -1\n",
    "\n",
    "def range_detections(row):\n",
    "    rownp = row.to_numpy()\n",
    "    rownonzero = np.nonzero(rownp)[0]\n",
    "    return rownonzero[-1] - rownonzero[0] + 1 if rownonzero.shape[0]>0 else -1\n",
    "\n",
    "def spatial_var(row):\n",
    "    rownp = row.to_numpy()\n",
    "    ids = np.repeat(np.argwhere(rownp>0), rownp[rownp>0])\n",
    "    return np.var(ids) if ids.shape[0]>0 else -1\n",
    "\n",
    "def spatial_std(row):\n",
    "    rownp = row.to_numpy()\n",
    "    ids = np.repeat(np.argwhere(rownp>0), rownp[rownp>0])\n",
    "    return np.std(ids) if ids.shape[0]>0 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfallmu.iloc[:, 3:3+n_inner_slices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for rforest\n",
    "my_features = [\"PEDetected_outer\", \"NActiveSlices_outer\", \"MeanNPEActive_outer\", \"SpatialVar_outer\", \"SpatialRange_outer\", \n",
    "               \"PEDetected_inner\", \"NActiveSlices_inner\", \"MeanNPEActive_inner\", \"SpatialVar_inner\", \"SpatialRange_inner\",\n",
    "               \"PEDetected_tot\", \"NActiveSlices_tot\", \"MeanNPEActive_tot\"]\n",
    "\n",
    "init = time.time()\n",
    "dfallmu[\"PEDetected_inner\"] = dfallmu.iloc[:, 3:3+n_inner_slices].apply(lambda row: pe_detected(row), axis=1)\n",
    "dfallmu[\"NActiveSlices_inner\"] = dfallmu.iloc[:, 3:3+n_inner_slices].apply(lambda row: nr_active_slices(row), axis=1)\n",
    "dfallmu[\"MeanNPEActive_inner\"] = dfallmu.iloc[:, 3:3+n_inner_slices].apply(lambda row: mean_npe_active(row), axis=1)\n",
    "dfallmu[\"StdNPEActive_inner\"] = dfallmu.iloc[:, 3:3+n_inner_slices].apply(lambda row: std_npe_active(row), axis=1)\n",
    "dfallmu[\"SpatialRange_inner\"] = dfallmu.iloc[:, 3:3+n_inner_slices].apply(lambda row: range_detections(row), axis=1)\n",
    "dfallmu[\"SpatialVar_inner\"] = dfallmu.iloc[:, 3:3+n_inner_slices].apply(lambda row: spatial_var(row), axis=1)\n",
    "dfallmu[\"SpatialStd_inner\"] = dfallmu.iloc[:, 3:3+n_inner_slices].apply(lambda row: spatial_std(row), axis=1)\n",
    "print(\"Compute Mu Inner features: {:.3f} sec\".format(time.time() - init))\n",
    "init = time.time()\n",
    "dfallmu[\"PEDetected_outer\"] = dfallmu.iloc[:, 3+n_inner_slices:3+n_inner_slices+n_outer_slices].apply(lambda row: pe_detected(row), axis=1)\n",
    "dfallmu[\"NActiveSlices_outer\"] = dfallmu.iloc[:, 3+n_inner_slices:3+n_inner_slices+n_outer_slices].apply(lambda row: nr_active_slices(row), axis=1)\n",
    "dfallmu[\"MeanNPEActive_outer\"] = dfallmu.iloc[:, 3+n_inner_slices:3+n_inner_slices+n_outer_slices].apply(lambda row: mean_npe_active(row), axis=1)\n",
    "dfallmu[\"StdNPEActive_outer\"] = dfallmu.iloc[:, 3+n_inner_slices:3+n_inner_slices+n_outer_slices].apply(lambda row: std_npe_active(row), axis=1)\n",
    "dfallmu[\"SpatialRange_outer\"] = dfallmu.iloc[:, 3+n_inner_slices:3+n_inner_slices+n_outer_slices].apply(lambda row: range_detections(row), axis=1)\n",
    "dfallmu[\"SpatialVar_outer\"] = dfallmu.iloc[:, 3+n_inner_slices:3+n_inner_slices+n_outer_slices].apply(lambda row: spatial_var(row), axis=1)\n",
    "dfallmu[\"SpatialStd_outer\"] = dfallmu.iloc[:, 3+n_inner_slices:3+n_inner_slices+n_outer_slices].apply(lambda row: spatial_std(row), axis=1)\n",
    "print(\"Compute Mu Outer features: {:.3f} sec\".format(time.time() - init))\n",
    "init = time.time()\n",
    "dfallmu[\"PEDetected_tot\"] = dfallmu[\"PEDetected_inner\"] + dfallmu[\"PEDetected_outer\"]\n",
    "dfallmu[\"NActiveSlices_tot\"] = dfallmu[\"NActiveSlices_inner\"] + dfallmu[\"NActiveSlices_outer\"]\n",
    "dfallmu[\"MeanNPEActive_tot\"] = dfallmu[\"PEDetected_tot\"] / dfallmu[\"NActiveSlices_tot\"]\n",
    "dfallmu = dfallmu.fillna(-1)\n",
    "print(\"Compute combined features: {:.3f} sec\".format(time.time() - init))\n",
    "\n",
    "init = time.time()\n",
    "dfallar39[\"PEDetected_inner\"] = dfallar39.iloc[:, 3:3+n_inner_slices].apply(lambda row: pe_detected(row), axis=1)\n",
    "dfallar39[\"NActiveSlices_inner\"] = dfallar39.iloc[:, 3:3+n_inner_slices].apply(lambda row: nr_active_slices(row), axis=1)\n",
    "dfallar39[\"MeanNPEActive_inner\"] = dfallar39.iloc[:, 3:3+n_inner_slices].apply(lambda row: mean_npe_active(row), axis=1)\n",
    "dfallar39[\"StdNPEActive_inner\"] = dfallar39.iloc[:, 3:3+n_inner_slices].apply(lambda row: std_npe_active(row), axis=1)\n",
    "dfallar39[\"SpatialRange_inner\"] = dfallar39.iloc[:, 3:3+n_inner_slices].apply(lambda row: range_detections(row), axis=1)\n",
    "dfallar39[\"SpatialVar_inner\"] = dfallar39.iloc[:, 3:3+n_inner_slices].apply(lambda row: spatial_var(row), axis=1)\n",
    "dfallar39[\"SpatialStd_inner\"] = dfallar39.iloc[:, 3:3+n_inner_slices].apply(lambda row: spatial_std(row), axis=1)\n",
    "print(\"Compute Ar39 Inner features: {:.3f} sec\".format(time.time() - init))\n",
    "init = time.time()\n",
    "dfallar39[\"PEDetected_outer\"] = dfallar39.iloc[:, 3+n_inner_slices:3+n_inner_slices+n_outer_slices].apply(lambda row: pe_detected(row), axis=1)\n",
    "dfallar39[\"NActiveSlices_outer\"] = dfallar39.iloc[:, 3+n_inner_slices:3+n_inner_slices+n_outer_slices].apply(lambda row: nr_active_slices(row), axis=1)\n",
    "dfallar39[\"MeanNPEActive_outer\"] = dfallar39.iloc[:, 3+n_inner_slices:3+n_inner_slices+n_outer_slices].apply(lambda row: mean_npe_active(row), axis=1)\n",
    "dfallar39[\"StdNPEActive_outer\"] = dfallar39.iloc[:, 3+n_inner_slices:3+n_inner_slices+n_outer_slices].apply(lambda row: std_npe_active(row), axis=1)\n",
    "dfallar39[\"SpatialRange_outer\"] = dfallar39.iloc[:, 3+n_inner_slices:3+n_inner_slices+n_outer_slices].apply(lambda row: range_detections(row), axis=1)\n",
    "dfallar39[\"SpatialVar_outer\"] = dfallar39.iloc[:, 3+n_inner_slices:3+n_inner_slices+n_outer_slices].apply(lambda row: spatial_var(row), axis=1)\n",
    "dfallar39[\"SpatialStd_outer\"] = dfallar39.iloc[:, 3+n_inner_slices:3+n_inner_slices+n_outer_slices].apply(lambda row: spatial_std(row), axis=1)\n",
    "print(\"Compute Ar39 Outer features: {:.3f} sec\".format(time.time() - init))\n",
    "init = time.time()\n",
    "dfallar39[\"PEDetected_tot\"] = dfallar39[\"PEDetected_inner\"] + dfallar39[\"PEDetected_outer\"]\n",
    "dfallar39[\"NActiveSlices_tot\"] = dfallar39[\"NActiveSlices_inner\"] + dfallar39[\"NActiveSlices_outer\"]\n",
    "dfallar39[\"MeanNPEActive_tot\"] = dfallar39[\"PEDetected_tot\"] / dfallar39[\"NActiveSlices_tot\"]\n",
    "dfallar39 = dfallar39.fillna(-1)\n",
    "print(\"Compute combined features: {:.3f} sec\".format(time.time() - init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([dfallar39, dfallmu], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RForest predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rf, y_rf = data.loc[:, my_features], data.loc[:, \"y\"]\n",
    "X_rf = np.array(X_rf)\n",
    "y_rf = np.array(y_rf)\n",
    "\n",
    "init = time.time()\n",
    "data[\"y_rf_pred\"] = rf.predict_proba(X_rf)[:, 1]\n",
    "print(\"[Info] RForest predict in {:.3f} seconds\".format(time.time() - init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cnn, y_cnn = data.iloc[:, 3:3+n_inner_slices+n_outer_slices], data.loc[:, \"y\"]\n",
    "X_cnn = np.array(X_cnn)\n",
    "X_cnn = tf.expand_dims(X_cnn, axis=-1)\n",
    "y_cnn = np.array(y_cnn)\n",
    "\n",
    "init = time.time()\n",
    "data[\"y_cnn_pred\"] = cnn.predict(X_cnn)\n",
    "print(\"[Info] CNN predict in {:.3f} seconds\".format(time.time() - init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preliminary cuts: 5 PE and 4 Out NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cut_5npe\"] = np.where(data.PEDetected_tot>=5, 1, 0)\n",
    "data[\"cut_4nas\"] = np.where(data.NActiveSlices_outer>=4, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combo predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"y_cut5npe_cnn\"] = np.where(data.cut_5npe==0, 0, data.y_cnn_pred)\n",
    "data[\"y_cut4nas_rf\"] = np.where(data.cut_4nas==0, 0, data.y_rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix as cv\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fpr, rf_tpr, rf_thresholds = roc_curve(data.y, data.y_cut4nas_rf)\n",
    "cnn_fpr, cnn_tpr, cnn_thresholds = roc_curve(data.y, data.y_cut5npe_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_tp = len(data[(data.y==1) & (data.y_cut5npe_cnn>0.50)])\n",
    "cnn_tn_1ar39 = len(data[(data.y==0) & (data.group==1) & (data.y_cut5npe_cnn<=0.50)])\n",
    "cnn_tn_2ar39 = len(data[(data.y==0) & (data.group==2) & (data.y_cut5npe_cnn<=0.50)])\n",
    "cnn_tn_3ar39 = len(data[(data.y==0) & (data.group==3) & (data.y_cut5npe_cnn<=0.50)])\n",
    "cnn_tn_4ar39 = len(data[(data.y==0) & (data.group==4) & (data.y_cut5npe_cnn<=0.50)])\n",
    "cnn_tn_5ar39 = len(data[(data.y==0) & (data.group==5) & (data.y_cut5npe_cnn<=0.50)])\n",
    "\n",
    "print(\"[Info] Per-class evaluation Cut 5 PE + CNN:\")\n",
    "print(\"\\t TPR: {:.5f}\".format(cnn_tp / len(data[data.y==1])))\n",
    "print(\"\\t TNR 1 Ar39: {:.5f}\".format(cnn_tn_1ar39 / len(data[data.group==1])))\n",
    "print(\"\\t TNR 2 Ar39: {:.5f}\".format(cnn_tn_2ar39 / len(data[data.group==2])))\n",
    "print(\"\\t TNR 3 Ar39: {:.5f}\".format(cnn_tn_3ar39 / len(data[data.group==3])))\n",
    "print(\"\\t TNR 4 Ar39: {:.5f}\".format(cnn_tn_4ar39 / len(data[data.group==4])))\n",
    "print(\"\\t TNR 5 Ar39: {:.5f}\".format(cnn_tn_5ar39 / len(data[data.group==5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tp = len(data[(data.y==1) & (data.y_cut4nas_rf>0.50)])\n",
    "rf_tn_1ar39 = len(data[(data.y==0) & (data.group==1) & (data.y_cut4nas_rf<=0.50)])\n",
    "rf_tn_2ar39 = len(data[(data.y==0) & (data.group==2) & (data.y_cut4nas_rf<=0.50)])\n",
    "rf_tn_3ar39 = len(data[(data.y==0) & (data.group==3) & (data.y_cut4nas_rf<=0.50)])\n",
    "rf_tn_4ar39 = len(data[(data.y==0) & (data.group==4) & (data.y_cut4nas_rf<=0.50)])\n",
    "rf_tn_5ar39 = len(data[(data.y==0) & (data.group==5) & (data.y_cut4nas_rf<=0.50)])\n",
    "\n",
    "print(\"[Info] Per-class evaluation Cut 5 PE + CNN:\")\n",
    "print(\"\\t TPR: {:.5f}\".format(rf_tp / len(data[data.y==1])))\n",
    "print(\"\\t TNR 1 Ar39: {:.5f}\".format(rf_tn_1ar39 / len(data[data.group==1])))\n",
    "print(\"\\t TNR 2 Ar39: {:.5f}\".format(rf_tn_2ar39 / len(data[data.group==2])))\n",
    "print(\"\\t TNR 3 Ar39: {:.5f}\".format(rf_tn_3ar39 / len(data[data.group==3])))\n",
    "print(\"\\t TNR 4 Ar39: {:.5f}\".format(rf_tn_4ar39 / len(data[data.group==4])))\n",
    "print(\"\\t TNR 5 Ar39: {:.5f}\".format(rf_tn_5ar39 / len(data[data.group==5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve\n",
    "To produce a good ROC curve, we cannot consider each background instance in the same way.\n",
    "We propose to estimate the final FPR as weighted mean of the FPR of each bg subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar39_probs = [0,\n",
    "              .980737931,\n",
    "              .0190753528,\n",
    "              .000185507806,\n",
    "              .00000120270894,\n",
    "              .00000000584817222]\n",
    "ar39_weights = [prob * sum(ar39_probs) for prob in ar39_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_threshold, max_threshold = 0, 1\n",
    "safe_increment = .1\n",
    "cnn_thresholds, cnn_tprs, cnn_fprs = [], [], []\n",
    "for threshold in np.linspace(min_threshold-safe_increment, \n",
    "                             max_threshold+safe_increment, 100):\n",
    "    cnn_tpr = len(data[(data.y==1) & (data.y_cut5npe_cnn>threshold)]) / len(data[(data.y==1)])\n",
    "    cnn_1fpr = len(data[(data.group==1) & (data.y_cut5npe_cnn>threshold)]) / len(data[(data.group==1)])\n",
    "    cnn_2fpr = len(data[(data.group==2) & (data.y_cut5npe_cnn>threshold)]) / len(data[(data.group==2)])\n",
    "    cnn_3fpr = len(data[(data.group==3) & (data.y_cut5npe_cnn>threshold)]) / len(data[(data.group==3)])\n",
    "    cnn_4fpr = len(data[(data.group==4) & (data.y_cut5npe_cnn>threshold)]) / len(data[(data.group==4)])\n",
    "    cnn_5fpr = len(data[(data.group==5) & (data.y_cut5npe_cnn>threshold)]) / len(data[(data.group==5)])\n",
    "    cnn_thresholds.append(threshold)\n",
    "    cnn_tprs.append(cnn_tpr)\n",
    "    cnn_fprs.append(ar39_weights[1] * cnn_1fpr + \\\n",
    "                    ar39_weights[2] * cnn_2fpr + \\\n",
    "                    ar39_weights[3] * cnn_3fpr + \\\n",
    "                    ar39_weights[4] * cnn_4fpr + \\\n",
    "                    ar39_weights[5] * cnn_5fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_thresholds, rf_tprs, rf_fprs = [], [], []\n",
    "for threshold in np.linspace(min_threshold-safe_increment, \n",
    "                             max_threshold+safe_increment, 100):\n",
    "    rf_tpr = len(data[(data.y==1) & (data.y_cut4nas_rf>threshold)]) / len(data[(data.y==1)])\n",
    "    rf_1fpr = len(data[(data.group==1) & (data.y_cut4nas_rf>threshold)]) / len(data[(data.group==1)])\n",
    "    rf_2fpr = len(data[(data.group==2) & (data.y_cut4nas_rf>threshold)]) / len(data[(data.group==2)])\n",
    "    rf_3fpr = len(data[(data.group==3) & (data.y_cut4nas_rf>threshold)]) / len(data[(data.group==3)])\n",
    "    rf_4fpr = len(data[(data.group==4) & (data.y_cut4nas_rf>threshold)]) / len(data[(data.group==4)])\n",
    "    rf_5fpr = len(data[(data.group==5) & (data.y_cut4nas_rf>threshold)]) / len(data[(data.group==5)])\n",
    "    rf_thresholds.append(threshold)\n",
    "    rf_tprs.append(rf_tpr)\n",
    "    rf_fprs.append(ar39_weights[1] * rf_1fpr + \\\n",
    "                   ar39_weights[2] * rf_2fpr + \\\n",
    "                   ar39_weights[3] * rf_3fpr + \\\n",
    "                   ar39_weights[4] * rf_4fpr + \\\n",
    "                   ar39_weights[5] * rf_5fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cnn_fprs, cnn_tprs, label=\"Cut 5PE + CNN\", linewidth=4)\n",
    "plt.plot(rf_fprs, rf_tprs, label=\"Cut 4NAS + RF\", linewidth=4)\n",
    "\n",
    "for t in [.5]:\n",
    "    id_cnn = np.argmin(np.abs(np.array(cnn_thresholds) - t))\n",
    "    plt.scatter(cnn_fprs[id_cnn], cnn_tprs[id_cnn], \n",
    "                label=\"Thr={}\".format(t), s=150, c='r')\n",
    "    print(\"[Info] Cut 5PE + CNN: Thr={:.1f} => TPR: {:.3f}%, FPR: {:.3f}%\".format(cnn_thresholds[id_cnn], \n",
    "                                                                           cnn_tprs[id_cnn] * 100, \n",
    "                                                                           cnn_fprs[id_cnn] * 100))\n",
    "for t in [.5]:\n",
    "    id_rf = np.argmin(np.abs(np.array(rf_thresholds) - t))\n",
    "    plt.scatter(rf_fprs[id_rf], rf_tprs[id_rf], \n",
    "               s=150, c='r')\n",
    "    print(\"[Info] Cut 4NAS + RF: Thr={:.1f} => TPR: {:.3f}%, FPR: {:.3f}%\".format(rf_thresholds[id_rf], \n",
    "                                                                           rf_tprs[id_rf] * 100, \n",
    "                                                                           rf_fprs[id_rf] * 100))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Positive Rate (weighted)\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlim(-.010, .50)\n",
    "plt.ylim(.50, .90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_thresholds = []\n",
    "cut_tprs = []\n",
    "cut_fprs = []\n",
    "for threshold in range(50):\n",
    "    tpr = len(data[(data.y==1) & (data.NActiveSlices_outer>threshold)]) / len(data[data.y==1])\n",
    "    fpr1 = len(data[(data.group==1) & (data.NActiveSlices_outer>threshold)]) / len(data[data.group==1])\n",
    "    fpr2 = len(data[(data.group==2) & (data.NActiveSlices_outer>threshold)]) / len(data[data.group==2])\n",
    "    fpr3 = len(data[(data.group==3) & (data.NActiveSlices_outer>threshold)]) / len(data[data.group==3])\n",
    "    fpr4 = len(data[(data.group==4) & (data.NActiveSlices_outer>threshold)]) / len(data[data.group==4])\n",
    "    fpr5 = len(data[(data.group==5) & (data.NActiveSlices_outer>threshold)]) / len(data[data.group==5])\n",
    "    cut_thresholds.append(threshold)\n",
    "    cut_tprs.append(tpr)\n",
    "    cut_fprs.append(ar39_weights[1] * fpr1 + \\\n",
    "                    ar39_weights[2] * fpr2 + \\\n",
    "                    ar39_weights[3] * fpr3 + \\\n",
    "                    ar39_weights[4] * fpr4 + \\\n",
    "                    ar39_weights[5] * fpr5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cut_fprs, cut_tprs, linewidth=4, label=\"Only Cut 4NAS\")\n",
    "plt.plot(rf_fprs, rf_tprs, linewidth=4, label=\"Cut 4NAS + RF\")\n",
    "\n",
    "for t in [.5]:\n",
    "    id_rf = np.argmin(np.abs(np.array(rf_thresholds) - t))\n",
    "    plt.scatter(rf_fprs[id_rf], rf_tprs[id_rf], \n",
    "                label=\"RF Thr={}\".format(t), s=200)\n",
    "    print(\"[Info] Cut 4NAS + RF: Thr={:.2f} => TPR: {:.3f}%, FPR: {:.3f}%\".format(rf_thresholds[id_rf], \n",
    "                                                                           rf_tprs[id_rf] * 100, \n",
    "                                                                           rf_fprs[id_rf] * 100))\n",
    "\n",
    "for t in [4]:\n",
    "    id_cut = np.argmin(np.abs(np.array(cut_thresholds) - t))\n",
    "    plt.scatter(cut_fprs[id_cut], cut_tprs[id_cut], \n",
    "                label=\"Thr={} NAS\".format(t), s=200)\n",
    "    print(\"[Info] Cut 4NAS: Thr={:.0f} NAS=> TPR: {:.3f}%, FPR: {:.3f}%\".format(cut_thresholds[id_cut], \n",
    "                                                                           cut_tprs[id_cut] * 100, \n",
    "                                                                           cut_fprs[id_cut] * 100))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Positive Rate (weighted)\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlim(-.01, .25)\n",
    "plt.ylim(.60, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_rates_training = [0.82367, 0.1813, 0.24757, 0.29246, 0.3635, 0.45465]\n",
    "plt.bar(range(len(true_rates_training)), true_rates_training)\n",
    "plt.bar(range(1, len(true_rates_training)), true_rates_training[1:], color=\"red\")\n",
    "plt.xticks(range(len(true_rates_training)), [\"Mu\", \"1 Ar39\",\n",
    "                                             \"2 Ar39\", \"3 Ar39\", \n",
    "                                             \"4 Ar39\", \"5 Ar39\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tpr = len(data[(data.y==1) & (data.y_cut4nas_rf>.5)]) / len(data[(data.y==1)])\n",
    "rf_fpr1 = len(data[(data.y==0) & (data.group==1) & (data.y_cut4nas_rf>.5)]) / len(data[(data.y==0) & (data.group==1)])\n",
    "rf_fpr2 = len(data[(data.y==0) & (data.group==2) & (data.y_cut4nas_rf>.5)]) / len(data[(data.y==0) & (data.group==2)])\n",
    "rf_fpr3 = len(data[(data.y==0) & (data.group==3) & (data.y_cut4nas_rf>.5)]) / len(data[(data.y==0) & (data.group==3)])\n",
    "rf_fpr4 = len(data[(data.y==0) & (data.group==4) & (data.y_cut4nas_rf>.5)]) / len(data[(data.y==0) & (data.group==4)])\n",
    "rf_fpr5 = len(data[(data.y==0) & (data.group==5) & (data.y_cut4nas_rf>.5)]) / len(data[(data.y==0) & (data.group==5)])\n",
    "true_rates_test = [rf_tpr, rf_fpr1, rf_fpr2, rf_fpr3, rf_fpr4, rf_fpr5]\n",
    "plt.bar(range(len(true_rates_test)), true_rates_test)\n",
    "plt.bar(range(1, len(true_rates_test)), true_rates_test[1:], color=\"red\")\n",
    "plt.xticks(range(len(true_rates_test)), [\"Mu\", \"1 Ar39\",\n",
    "                                             \"2 Ar39\", \"3 Ar39\", \n",
    "                                             \"4 Ar39\", \"5 Ar39\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_rates_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
